{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "BfHN7LajAiTT"
   },
   "source": [
    "## Deep Learning - Neural Networks from Scratch\n",
    "\n",
    "Member 1: Anish Batra, ab8166\n",
    "\n",
    "Member 2: Prashant Mahajan,  prm349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4z9DncmAiTZ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import glob\n",
    "import sys\n",
    "# you shouldn't need to make any more imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "VGP7FMFJAiTk"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j65EBOosAiTn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8C8EBctIAiTu",
    "outputId": "6d01e10c-6bb8-4be9-e5c9-a361ad251e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8Bu1T0StZF6H",
    "outputId": "0d9d80f3-0edd-4bfe-9961-d3754decba3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000,)\n",
      "(10000, 3, 32, 32)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# convert data to numpy arrays\n",
    "train_images = (trainset.train_data.reshape(50000, 3, 32, 32)) / 255.0\n",
    "train_labels = np.array(trainset.train_labels).reshape(50000, )\n",
    "\n",
    "test_images = (testset.test_data.reshape(10000, 3, 32, 32)) / 255.0\n",
    "test_labels = np.array(testset.test_labels).reshape(10000, )\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "8pAFGUl7AiT0"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRKt3EBYAiT2"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of neural network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    Provides necessary functions for training and prediction. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases for each layer\n",
    "        :param layer_dimensions: (list) number of nodes in each layer\n",
    "        :param drop_prob: drop probability for dropout layers. Only required in part 2 of the assignment\n",
    "        :param reg_lambda: regularization parameter. Only required in part 2 of the assignment\n",
    "        \"\"\"\n",
    "        np.random.seed(1)\n",
    "\n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)\n",
    "        self.drop_prob = drop_prob\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.linear_cache = {}\n",
    "        self.activation_cache = {}\n",
    "        self.dropout_mask = {}\n",
    "\n",
    "        # init parameters\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.parameters['W' + str(l)] = np.random.normal(\n",
    "                0, 7.5e-02, (layer_dimensions[l - 1], layer_dimensions[l]))\n",
    "            self.parameters['b' + str(l)] = np.zeros(layer_dimensions[l])\n",
    "\n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        m = A.shape[0]\n",
    "        out = np.dot(np.reshape(A, [m, -1]), W) + b\n",
    "        cache = (A, W, b)\n",
    "        return out, cache\n",
    "\n",
    "    def activationForward(self, A, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Common interface to access all activation functions.\n",
    "        :param A: input to the activation function\n",
    "        :param prob: activation funciton to apply to A. Just \"relu\" for this assignment.\n",
    "        :returns: activation(A)\n",
    "        \"\"\"\n",
    "        if activation == 'relu':\n",
    "            out = self.relu(A)\n",
    "        return out\n",
    "\n",
    "    def relu(self, X):\n",
    "        out = np.maximum(X, 0)\n",
    "        return out\n",
    "\n",
    "    def dropout(self, A, prob):\n",
    "        \"\"\"\n",
    "        :param A: Activation\n",
    "        :param prob: drop prob\n",
    "        :returns: tuple (A, M) \n",
    "            WHERE\n",
    "            A is matrix after applying dropout\n",
    "            M is dropout mask, used in the backward pass\n",
    "        \"\"\"\n",
    "        M = np.random.rand(A.shape[0], A.shape[1])\n",
    "        M = (M > prob) * 1.0\n",
    "        M /= (1 - prob)\n",
    "        A *= M\n",
    "\n",
    "        return A, M\n",
    "\n",
    "    def forwardPropagation(self, X):\n",
    "        \"\"\"\n",
    "        Runs an input X through the neural network to compute activations\n",
    "        for all layers. Returns the output computed at the last layer along\n",
    "        with the cache required for backpropagation.\n",
    "        :returns: (tuple) AL, cache\n",
    "            WHERE \n",
    "            AL is activation of last layer\n",
    "            cache is cached values for each layer that\n",
    "                     are needed in further steps\n",
    "        \"\"\"\n",
    "        size = X.shape[0]\n",
    "        X = np.reshape(X, [size, -1])\n",
    "\n",
    "        no_layers = self.num_layers\n",
    "\n",
    "        for layer in range(1, no_layers - 1):\n",
    "            Z, self.linear_cache[str(layer)] = self.affineForward(\n",
    "                X, self.parameters['W' + str(layer)],\n",
    "                self.parameters['b' + str(layer)])\n",
    "            self.activation_cache[str(layer)] = Z.copy()\n",
    "            A = self.activationForward(Z, activation='relu')\n",
    "\n",
    "            if self.drop_prob > 0:\n",
    "                A, self.dropout_mask[str(layer)] = self.dropout(\n",
    "                    A, self.drop_prob)\n",
    "            X = A.copy()\n",
    "\n",
    "        AL, cache = self.affineForward(\n",
    "            X, self.parameters['W' + str(no_layers - 1)],\n",
    "            self.parameters['b' + str(no_layers - 1)])\n",
    "\n",
    "        return AL, cache\n",
    "\n",
    "    def softmax(self, Z):\n",
    "        AL_normal = Z - np.max(Z, axis=1, keepdims=True)\n",
    "        K = np.sum(np.exp(AL_normal), axis=1, keepdims=True)\n",
    "        log_probability = AL_normal - np.log(K)\n",
    "        A = np.exp(log_probability)\n",
    "        return A, log_probability\n",
    "\n",
    "    def costFunction(self, AL, y):\n",
    "        \"\"\"\n",
    "        :param AL: Activation of last layer, shape (num_classes, S)\n",
    "        :param y: labels, shape (S)\n",
    "        :param alpha: regularization parameter\n",
    "        :returns cost, dAL: A scalar denoting cost and the gradient of cost\n",
    "        \"\"\"\n",
    "        # compute cost\n",
    "        N = AL.shape[0]\n",
    "        A, log_probability = self.softmax(AL)\n",
    "        cost = -np.sum(log_probability[np.arange(N), y]) / N\n",
    "\n",
    "        if self.reg_lambda > 0:\n",
    "            # add regularization\n",
    "            for l in range(self.num_layers - 1, 0, -1):\n",
    "                cost += 0.5 * self.reg_lambda * (np.sum(\n",
    "                    np.square(self.parameters['W' + str(l)])))\n",
    "\n",
    "        # gradient of cost\n",
    "        dAL = A.copy()\n",
    "        dAL[np.arange(N), y] -= 1\n",
    "        dAL /= N\n",
    "\n",
    "        return cost, dAL\n",
    "\n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the affine layer.\n",
    "        :param dA_prev: gradient from the next layer.\n",
    "        :param cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        X, W, b = cache\n",
    "        dA, dW, db = None, None, None\n",
    "        n = X.shape[0]\n",
    "        m = np.reshape(X, [n, -1])\n",
    "        dA = np.dot(dA_prev, W.T)\n",
    "        dA = np.reshape(dA, X.shape)\n",
    "        dW = np.dot(m.T, dA_prev)\n",
    "        db = np.sum(dA_prev, axis=0)\n",
    "\n",
    "        return dA, dW, db\n",
    "\n",
    "    def activationBackward(self, dA, cache, activation=\"relu\"):\n",
    "        \"\"\"\n",
    "        Interface to call backward on activation functions.\n",
    "        In this case, it's just relu. \n",
    "        \"\"\"\n",
    "        if activation == 'relu':\n",
    "            out = self.relu_derivative(dA, cache)\n",
    "        return out\n",
    "\n",
    "    def relu_derivative(self, dx, activation_cache):\n",
    "        dout, x = None, activation_cache\n",
    "        relu_mask = (x >= 0)\n",
    "        dout = dx * relu_mask\n",
    "        return dout\n",
    "\n",
    "    def dropout_backward(self, dA, cache):\n",
    "        dA *= cache\n",
    "        return dA\n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache):\n",
    "        \"\"\"\n",
    "        Run backpropagation to compute gradients on all paramters in the model\n",
    "        :param dAL: gradient on the last layer of the network. Returned by the cost function.\n",
    "        :param Y: labels\n",
    "        :param cache: cached values during forwardprop\n",
    "        :returns gradients: dW and db for each weight/bias\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "\n",
    "        dXL, dWL, dbL = self.affineBackward(dAL, cache)\n",
    "        n = self.num_layers\n",
    "\n",
    "        gradients['W' + str(n - 1)] = dWL\n",
    "        gradients['b' + str(n - 1)] = dbL\n",
    "\n",
    "        if self.reg_lambda > 0:\n",
    "            gradients['W' + str(\n",
    "                n - 1)] += self.reg_lambda * self.parameters['W' + str(n - 1)]\n",
    "\n",
    "        for l in range(n - 2, 0, -1):\n",
    "            reluBack = self.activationBackward(\n",
    "                dXL, self.activation_cache[str(l)], activation='relu')\n",
    "\n",
    "            if self.drop_prob > 0:\n",
    "                reluBack = self.dropout_backward(reluBack,\n",
    "                                                 self.dropout_mask[str(l)])\n",
    "\n",
    "            dXL, dWL, dbL = self.affineBackward(reluBack,\n",
    "                                                self.linear_cache[str(l)])\n",
    "\n",
    "            if self.reg_lambda > 0:\n",
    "                gradients['W' + str(\n",
    "                    l)] = dWL + self.reg_lambda * self.parameters['W' + str(l)]\n",
    "            else:\n",
    "                gradients['W' + str(l)] = dWL\n",
    "            gradients['b' + str(l)] = dbL\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def updateParameters(self, gradients, alpha):\n",
    "        \"\"\"\n",
    "        :param gradients: gradients for each weight/bias\n",
    "        :param alpha: step size for gradient descent \n",
    "        \"\"\"\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.parameters['W' + str(l)] = self.parameters[\n",
    "                'W' + str(l)] - alpha * gradients['W' + str(l)]\n",
    "            self.parameters['b' + str(l)] = self.parameters[\n",
    "                'b' + str(l)] - alpha * gradients['b' + str(l)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for each sample\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        AL, cache = self.forwardPropagation(X)\n",
    "        y_pred.append(np.argmax(AL, axis=1))\n",
    "        return y_pred\n",
    "\n",
    "    def train(self,\n",
    "              X,\n",
    "              y,\n",
    "              iters=1000,\n",
    "              alpha=0.0001,\n",
    "              batch_size=100,\n",
    "              print_every=100):\n",
    "        \"\"\"\n",
    "        :param X: input samples, each column is a sample\n",
    "        :param y: labels for input samples, y.shape[0] must equal X.shape[1]\n",
    "        :param iters: number of training iterations\n",
    "        :param alpha: step size for gradient descent\n",
    "        :param batch_size: number of samples in a minibatch\n",
    "        :param print_every: no. of iterations to print debug info after\n",
    "        \"\"\"\n",
    "\n",
    "        # Split input data into training and validation set\n",
    "        X_train, y_train, X_val, y_val = self.dataset_split(X, y, 0.9)\n",
    "\n",
    "        for i in range(0, iters):\n",
    "            # get minibatch\n",
    "            X_batch, y_batch = self.get_batch(X_train, y_train, batch_size)\n",
    "\n",
    "            # forward prop\n",
    "            AL, cache = self.forwardPropagation(X_batch)\n",
    "\n",
    "            # compute loss\n",
    "            cost, dAL = self.costFunction(AL, y_batch)\n",
    "\n",
    "            # compute gradients\n",
    "            gradients = self.backPropagation(dAL, y_batch, cache)\n",
    "\n",
    "            # update weights and biases based on gradient\n",
    "            self.updateParameters(gradients, alpha)\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                # print cost, train and validation set accuracies\n",
    "                AL_val, cache = self.forwardPropagation(X_val)\n",
    "                print(\n",
    "                    \"Iteration Number: %d | Cost: %.4f | Train Accuracy: %.4f | Validation Accuracy: %.4f\"\n",
    "                    % (i, cost, self.accuracy(np.argmax(AL, axis=1), y_batch),\n",
    "                       self.accuracy(np.argmax(AL_val, axis=1), y_val)))\n",
    "\n",
    "    def dataset_split(self, X, y, ratio=0.8):\n",
    "        \"\"\"\n",
    "        function to split the training data into training and validation\n",
    "        \n",
    "        :param X: input samples\n",
    "        :param y: labels\n",
    "        :param ratio: ratio of training data from total data\n",
    "        :returns: training data and validation data\n",
    "        \"\"\"\n",
    "        split = int(ratio * X.shape[0])\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        training_idx, val_idx = indices[:split], indices[split:]\n",
    "        X_train, X_val = X[training_idx, ...], X[val_idx, ...]\n",
    "        y_train, y_val = y[training_idx], y[val_idx]\n",
    "\n",
    "        return X_train, y_train, X_val, y_val\n",
    "\n",
    "    def get_batch(self, X, y, batch_size):\n",
    "        \"\"\"\n",
    "        Return minibatch of samples and labels\n",
    "        \n",
    "        :param X, y: samples and corresponding labels\n",
    "        :parma batch_size: minibatch size\n",
    "        :returns: (tuple) X_batch, y_batch\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        mask = np.random.choice(num_train, batch_size)\n",
    "        X_batch = X[mask]\n",
    "        y_batch = y[mask]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def accuracy(self, y_pred, y_test):\n",
    "        '''\n",
    "        This function tests the accuracy of the trained model\n",
    "        :param y_pred: Predicted class \n",
    "        :param y_test: Actual Class\n",
    "        :returns: accuracy\n",
    "        '''\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        accuracy = np.mean(y_pred == y_test, axis=0)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duqaeKbgsAWG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers:  7 \n",
      "\n",
      "Weight 1 shape:  (3072, 800)\n",
      "  Bias 1 shape:  (800,)\n",
      "Weight 2 shape:  (800, 400)\n",
      "  Bias 2 shape:  (400,)\n",
      "Weight 3 shape:  (400, 200)\n",
      "  Bias 3 shape:  (200,)\n",
      "Weight 4 shape:  (200, 100)\n",
      "  Bias 4 shape:  (100,)\n",
      "Weight 5 shape:  (100, 50)\n",
      "  Bias 5 shape:  (50,)\n",
      "Weight 6 shape:  (50, 10)\n",
      "  Bias 6 shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "network = NeuralNetwork([32 * 32 * 3, 800, 400, 200, 100, 50, 10])\n",
    "\n",
    "print(\"Number of layers: \", network.num_layers, \"\\n\")\n",
    "\n",
    "for i in range(1, network.num_layers):\n",
    "    print(\"Weight\", i, \"shape: \", network.parameters['W' + str(i)].shape)\n",
    "    print(\"  Bias\", i, \"shape: \", network.parameters['b' + str(i)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "\n",
    "- Learning rate: 0.01\n",
    "- Batch size: 128\n",
    "- Number of iterations: 20,000\n",
    "- Print after every: 100 iterations\n",
    "\n",
    "---\n",
    "\n",
    "- Training on 45,000 images (90%)\n",
    "- Validating on 5,000 images (10%)\n",
    "- Testing on 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "colab_type": "code",
    "id": "BW5z7OkUsFlq",
    "outputId": "181562ec-53e2-4296-fb81-5e862c1553e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 0 | Cost: 2.4179 | Train Accuracy: 0.1250 | Validation Accuracy: 0.1020\n",
      "Iteration Number: 100 | Cost: 2.1229 | Train Accuracy: 0.2812 | Validation Accuracy: 0.2258\n",
      "Iteration Number: 200 | Cost: 1.9579 | Train Accuracy: 0.2969 | Validation Accuracy: 0.2886\n",
      "Iteration Number: 300 | Cost: 1.9613 | Train Accuracy: 0.3203 | Validation Accuracy: 0.3066\n",
      "Iteration Number: 400 | Cost: 2.0452 | Train Accuracy: 0.2344 | Validation Accuracy: 0.3284\n",
      "Iteration Number: 500 | Cost: 1.8203 | Train Accuracy: 0.3203 | Validation Accuracy: 0.3412\n",
      "Iteration Number: 600 | Cost: 1.9407 | Train Accuracy: 0.3281 | Validation Accuracy: 0.3306\n",
      "Iteration Number: 700 | Cost: 1.7641 | Train Accuracy: 0.3828 | Validation Accuracy: 0.3098\n",
      "Iteration Number: 800 | Cost: 1.7331 | Train Accuracy: 0.4219 | Validation Accuracy: 0.3580\n",
      "Iteration Number: 900 | Cost: 1.8094 | Train Accuracy: 0.3359 | Validation Accuracy: 0.3558\n",
      "Iteration Number: 1000 | Cost: 1.5634 | Train Accuracy: 0.4609 | Validation Accuracy: 0.3648\n",
      "Iteration Number: 1100 | Cost: 1.9112 | Train Accuracy: 0.3125 | Validation Accuracy: 0.3446\n",
      "Iteration Number: 1200 | Cost: 1.7552 | Train Accuracy: 0.3516 | Validation Accuracy: 0.3892\n",
      "Iteration Number: 1300 | Cost: 1.7564 | Train Accuracy: 0.3594 | Validation Accuracy: 0.3810\n",
      "Iteration Number: 1400 | Cost: 1.6336 | Train Accuracy: 0.3359 | Validation Accuracy: 0.3948\n",
      "Iteration Number: 1500 | Cost: 1.6759 | Train Accuracy: 0.4453 | Validation Accuracy: 0.3954\n",
      "Iteration Number: 1600 | Cost: 1.5355 | Train Accuracy: 0.3906 | Validation Accuracy: 0.3760\n",
      "Iteration Number: 1700 | Cost: 1.5687 | Train Accuracy: 0.4922 | Validation Accuracy: 0.3896\n",
      "Iteration Number: 1800 | Cost: 1.5618 | Train Accuracy: 0.4531 | Validation Accuracy: 0.4090\n",
      "Iteration Number: 1900 | Cost: 1.7383 | Train Accuracy: 0.4141 | Validation Accuracy: 0.3968\n",
      "Iteration Number: 2000 | Cost: 1.4589 | Train Accuracy: 0.4141 | Validation Accuracy: 0.3856\n",
      "Iteration Number: 2100 | Cost: 1.5995 | Train Accuracy: 0.4141 | Validation Accuracy: 0.4168\n",
      "Iteration Number: 2200 | Cost: 1.6194 | Train Accuracy: 0.3906 | Validation Accuracy: 0.4068\n",
      "Iteration Number: 2300 | Cost: 1.5203 | Train Accuracy: 0.4844 | Validation Accuracy: 0.4180\n",
      "Iteration Number: 2400 | Cost: 1.6343 | Train Accuracy: 0.4297 | Validation Accuracy: 0.4264\n",
      "Iteration Number: 2500 | Cost: 1.5937 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4316\n",
      "Iteration Number: 2600 | Cost: 1.5595 | Train Accuracy: 0.4297 | Validation Accuracy: 0.4262\n",
      "Iteration Number: 2700 | Cost: 1.3785 | Train Accuracy: 0.5000 | Validation Accuracy: 0.4310\n",
      "Iteration Number: 2800 | Cost: 1.7440 | Train Accuracy: 0.4062 | Validation Accuracy: 0.4308\n",
      "Iteration Number: 2900 | Cost: 1.5144 | Train Accuracy: 0.4375 | Validation Accuracy: 0.4284\n",
      "Iteration Number: 3000 | Cost: 1.7321 | Train Accuracy: 0.4219 | Validation Accuracy: 0.4226\n",
      "Iteration Number: 3100 | Cost: 1.6150 | Train Accuracy: 0.3828 | Validation Accuracy: 0.4136\n",
      "Iteration Number: 3200 | Cost: 1.4615 | Train Accuracy: 0.4531 | Validation Accuracy: 0.4374\n",
      "Iteration Number: 3300 | Cost: 1.5287 | Train Accuracy: 0.4375 | Validation Accuracy: 0.4412\n",
      "Iteration Number: 3400 | Cost: 1.2713 | Train Accuracy: 0.5547 | Validation Accuracy: 0.4518\n",
      "Iteration Number: 3500 | Cost: 1.6447 | Train Accuracy: 0.4453 | Validation Accuracy: 0.4444\n",
      "Iteration Number: 3600 | Cost: 1.5234 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4296\n",
      "Iteration Number: 3700 | Cost: 1.5237 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4284\n",
      "Iteration Number: 3800 | Cost: 1.4970 | Train Accuracy: 0.4844 | Validation Accuracy: 0.4426\n",
      "Iteration Number: 3900 | Cost: 1.3443 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4476\n",
      "Iteration Number: 4000 | Cost: 1.5546 | Train Accuracy: 0.4688 | Validation Accuracy: 0.4550\n",
      "Iteration Number: 4100 | Cost: 1.4936 | Train Accuracy: 0.4453 | Validation Accuracy: 0.4512\n",
      "Iteration Number: 4200 | Cost: 1.3172 | Train Accuracy: 0.5391 | Validation Accuracy: 0.4472\n",
      "Iteration Number: 4300 | Cost: 1.5539 | Train Accuracy: 0.4688 | Validation Accuracy: 0.4518\n",
      "Iteration Number: 4400 | Cost: 1.5338 | Train Accuracy: 0.4219 | Validation Accuracy: 0.4566\n",
      "Iteration Number: 4500 | Cost: 1.4403 | Train Accuracy: 0.5000 | Validation Accuracy: 0.4622\n",
      "Iteration Number: 4600 | Cost: 1.5618 | Train Accuracy: 0.4531 | Validation Accuracy: 0.4484\n",
      "Iteration Number: 4700 | Cost: 1.4298 | Train Accuracy: 0.4922 | Validation Accuracy: 0.4714\n",
      "Iteration Number: 4800 | Cost: 1.3967 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4700\n",
      "Iteration Number: 4900 | Cost: 1.6100 | Train Accuracy: 0.4688 | Validation Accuracy: 0.4698\n",
      "Iteration Number: 5000 | Cost: 1.3243 | Train Accuracy: 0.5156 | Validation Accuracy: 0.4638\n",
      "Iteration Number: 5100 | Cost: 1.3192 | Train Accuracy: 0.5469 | Validation Accuracy: 0.4584\n",
      "Iteration Number: 5200 | Cost: 1.4195 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4604\n",
      "Iteration Number: 5300 | Cost: 1.3379 | Train Accuracy: 0.4844 | Validation Accuracy: 0.4694\n",
      "Iteration Number: 5400 | Cost: 1.3230 | Train Accuracy: 0.5156 | Validation Accuracy: 0.4638\n",
      "Iteration Number: 5500 | Cost: 1.3267 | Train Accuracy: 0.5156 | Validation Accuracy: 0.4692\n",
      "Iteration Number: 5600 | Cost: 1.1985 | Train Accuracy: 0.5938 | Validation Accuracy: 0.4784\n",
      "Iteration Number: 5700 | Cost: 1.4752 | Train Accuracy: 0.4844 | Validation Accuracy: 0.4700\n",
      "Iteration Number: 5800 | Cost: 1.2662 | Train Accuracy: 0.5703 | Validation Accuracy: 0.4604\n",
      "Iteration Number: 5900 | Cost: 1.4732 | Train Accuracy: 0.5000 | Validation Accuracy: 0.4810\n",
      "Iteration Number: 6000 | Cost: 1.3418 | Train Accuracy: 0.5312 | Validation Accuracy: 0.4842\n",
      "Iteration Number: 6100 | Cost: 1.4398 | Train Accuracy: 0.5156 | Validation Accuracy: 0.4574\n",
      "Iteration Number: 6200 | Cost: 1.3315 | Train Accuracy: 0.5000 | Validation Accuracy: 0.4866\n",
      "Iteration Number: 6300 | Cost: 1.3731 | Train Accuracy: 0.5000 | Validation Accuracy: 0.4818\n",
      "Iteration Number: 6400 | Cost: 1.2993 | Train Accuracy: 0.5391 | Validation Accuracy: 0.4838\n",
      "Iteration Number: 6500 | Cost: 1.2703 | Train Accuracy: 0.5859 | Validation Accuracy: 0.4800\n",
      "Iteration Number: 6600 | Cost: 1.3754 | Train Accuracy: 0.4844 | Validation Accuracy: 0.4778\n",
      "Iteration Number: 6700 | Cost: 1.2812 | Train Accuracy: 0.5469 | Validation Accuracy: 0.4860\n",
      "Iteration Number: 6800 | Cost: 1.3134 | Train Accuracy: 0.4844 | Validation Accuracy: 0.4738\n",
      "Iteration Number: 6900 | Cost: 1.3156 | Train Accuracy: 0.5234 | Validation Accuracy: 0.4620\n",
      "Iteration Number: 7000 | Cost: 1.3131 | Train Accuracy: 0.6016 | Validation Accuracy: 0.4900\n",
      "Iteration Number: 7100 | Cost: 1.2431 | Train Accuracy: 0.6016 | Validation Accuracy: 0.4716\n",
      "Iteration Number: 7200 | Cost: 1.3584 | Train Accuracy: 0.5078 | Validation Accuracy: 0.4648\n",
      "Iteration Number: 7300 | Cost: 1.3877 | Train Accuracy: 0.4766 | Validation Accuracy: 0.4458\n",
      "Iteration Number: 7400 | Cost: 1.1838 | Train Accuracy: 0.5938 | Validation Accuracy: 0.4712\n",
      "Iteration Number: 7500 | Cost: 1.3675 | Train Accuracy: 0.5625 | Validation Accuracy: 0.4860\n",
      "Iteration Number: 7600 | Cost: 1.5496 | Train Accuracy: 0.4922 | Validation Accuracy: 0.4706\n",
      "Iteration Number: 7700 | Cost: 1.1876 | Train Accuracy: 0.5078 | Validation Accuracy: 0.4856\n",
      "Iteration Number: 7800 | Cost: 1.2729 | Train Accuracy: 0.5391 | Validation Accuracy: 0.4894\n",
      "Iteration Number: 7900 | Cost: 1.1438 | Train Accuracy: 0.6406 | Validation Accuracy: 0.4864\n",
      "Iteration Number: 8000 | Cost: 1.0999 | Train Accuracy: 0.6016 | Validation Accuracy: 0.4860\n",
      "Iteration Number: 8100 | Cost: 1.1929 | Train Accuracy: 0.5625 | Validation Accuracy: 0.4800\n",
      "Iteration Number: 8200 | Cost: 1.3962 | Train Accuracy: 0.5547 | Validation Accuracy: 0.4952\n",
      "Iteration Number: 8300 | Cost: 1.0762 | Train Accuracy: 0.6719 | Validation Accuracy: 0.4932\n",
      "Iteration Number: 8400 | Cost: 1.0742 | Train Accuracy: 0.5859 | Validation Accuracy: 0.4786\n",
      "Iteration Number: 8500 | Cost: 1.2637 | Train Accuracy: 0.5391 | Validation Accuracy: 0.4800\n",
      "Iteration Number: 8600 | Cost: 1.2178 | Train Accuracy: 0.5469 | Validation Accuracy: 0.4934\n",
      "Iteration Number: 8700 | Cost: 1.2133 | Train Accuracy: 0.5469 | Validation Accuracy: 0.4928\n",
      "Iteration Number: 8800 | Cost: 1.2094 | Train Accuracy: 0.5391 | Validation Accuracy: 0.4974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 8900 | Cost: 1.1343 | Train Accuracy: 0.6094 | Validation Accuracy: 0.4740\n",
      "Iteration Number: 9000 | Cost: 1.3333 | Train Accuracy: 0.5234 | Validation Accuracy: 0.4942\n",
      "Iteration Number: 9100 | Cost: 1.2425 | Train Accuracy: 0.5312 | Validation Accuracy: 0.4906\n",
      "Iteration Number: 9200 | Cost: 1.2026 | Train Accuracy: 0.6016 | Validation Accuracy: 0.4854\n",
      "Iteration Number: 9300 | Cost: 1.2227 | Train Accuracy: 0.6094 | Validation Accuracy: 0.4984\n",
      "Iteration Number: 9400 | Cost: 1.2718 | Train Accuracy: 0.5703 | Validation Accuracy: 0.4986\n",
      "Iteration Number: 9500 | Cost: 1.1551 | Train Accuracy: 0.5781 | Validation Accuracy: 0.4940\n",
      "Iteration Number: 9600 | Cost: 1.2254 | Train Accuracy: 0.5625 | Validation Accuracy: 0.5066\n",
      "Iteration Number: 9700 | Cost: 1.0973 | Train Accuracy: 0.5781 | Validation Accuracy: 0.4962\n",
      "Iteration Number: 9800 | Cost: 1.0772 | Train Accuracy: 0.6328 | Validation Accuracy: 0.4930\n",
      "Iteration Number: 9900 | Cost: 1.2813 | Train Accuracy: 0.5312 | Validation Accuracy: 0.4744\n",
      "Iteration Number: 10000 | Cost: 1.2138 | Train Accuracy: 0.5547 | Validation Accuracy: 0.4822\n",
      "Iteration Number: 10100 | Cost: 1.1756 | Train Accuracy: 0.5859 | Validation Accuracy: 0.4632\n",
      "Iteration Number: 10200 | Cost: 1.2035 | Train Accuracy: 0.5938 | Validation Accuracy: 0.4940\n",
      "Iteration Number: 10300 | Cost: 1.0739 | Train Accuracy: 0.6172 | Validation Accuracy: 0.5006\n",
      "Iteration Number: 10400 | Cost: 1.1017 | Train Accuracy: 0.6172 | Validation Accuracy: 0.4922\n",
      "Iteration Number: 10500 | Cost: 1.1469 | Train Accuracy: 0.6094 | Validation Accuracy: 0.4808\n",
      "Iteration Number: 10600 | Cost: 1.2452 | Train Accuracy: 0.5469 | Validation Accuracy: 0.4944\n",
      "Iteration Number: 10700 | Cost: 1.0911 | Train Accuracy: 0.5703 | Validation Accuracy: 0.4964\n",
      "Iteration Number: 10800 | Cost: 1.2386 | Train Accuracy: 0.5469 | Validation Accuracy: 0.4748\n",
      "Iteration Number: 10900 | Cost: 1.0562 | Train Accuracy: 0.5859 | Validation Accuracy: 0.4756\n",
      "Iteration Number: 11000 | Cost: 1.0630 | Train Accuracy: 0.5859 | Validation Accuracy: 0.5062\n",
      "Iteration Number: 11100 | Cost: 1.0569 | Train Accuracy: 0.6641 | Validation Accuracy: 0.5032\n",
      "Iteration Number: 11200 | Cost: 1.0521 | Train Accuracy: 0.6016 | Validation Accuracy: 0.5014\n",
      "Iteration Number: 11300 | Cost: 1.0892 | Train Accuracy: 0.6094 | Validation Accuracy: 0.4970\n",
      "Iteration Number: 11400 | Cost: 1.2022 | Train Accuracy: 0.6562 | Validation Accuracy: 0.4808\n",
      "Iteration Number: 11500 | Cost: 1.2520 | Train Accuracy: 0.5547 | Validation Accuracy: 0.4876\n",
      "Iteration Number: 11600 | Cost: 1.0155 | Train Accuracy: 0.5938 | Validation Accuracy: 0.5028\n",
      "Iteration Number: 11700 | Cost: 1.1910 | Train Accuracy: 0.5938 | Validation Accuracy: 0.4988\n",
      "Iteration Number: 11800 | Cost: 1.0695 | Train Accuracy: 0.6172 | Validation Accuracy: 0.4796\n",
      "Iteration Number: 11900 | Cost: 1.2370 | Train Accuracy: 0.5078 | Validation Accuracy: 0.5014\n",
      "Iteration Number: 12000 | Cost: 1.1266 | Train Accuracy: 0.5938 | Validation Accuracy: 0.4810\n",
      "Iteration Number: 12100 | Cost: 1.1764 | Train Accuracy: 0.5703 | Validation Accuracy: 0.5006\n",
      "Iteration Number: 12200 | Cost: 0.9887 | Train Accuracy: 0.6719 | Validation Accuracy: 0.5010\n",
      "Iteration Number: 12300 | Cost: 1.1430 | Train Accuracy: 0.6328 | Validation Accuracy: 0.4908\n",
      "Iteration Number: 12400 | Cost: 1.0909 | Train Accuracy: 0.6016 | Validation Accuracy: 0.5086\n",
      "Iteration Number: 12500 | Cost: 1.1839 | Train Accuracy: 0.5625 | Validation Accuracy: 0.4910\n",
      "Iteration Number: 12600 | Cost: 1.1122 | Train Accuracy: 0.6484 | Validation Accuracy: 0.5108\n",
      "Iteration Number: 12700 | Cost: 1.1071 | Train Accuracy: 0.5938 | Validation Accuracy: 0.4952\n",
      "Iteration Number: 12800 | Cost: 1.0666 | Train Accuracy: 0.6250 | Validation Accuracy: 0.4906\n",
      "Iteration Number: 12900 | Cost: 0.9521 | Train Accuracy: 0.6484 | Validation Accuracy: 0.5080\n",
      "Iteration Number: 13000 | Cost: 0.9585 | Train Accuracy: 0.6641 | Validation Accuracy: 0.5072\n",
      "Iteration Number: 13100 | Cost: 1.1812 | Train Accuracy: 0.5781 | Validation Accuracy: 0.4636\n",
      "Iteration Number: 13200 | Cost: 1.0163 | Train Accuracy: 0.6562 | Validation Accuracy: 0.5092\n",
      "Iteration Number: 13300 | Cost: 1.2434 | Train Accuracy: 0.6094 | Validation Accuracy: 0.4794\n",
      "Iteration Number: 13400 | Cost: 0.9643 | Train Accuracy: 0.6406 | Validation Accuracy: 0.5032\n",
      "Iteration Number: 13500 | Cost: 1.2909 | Train Accuracy: 0.5234 | Validation Accuracy: 0.4914\n",
      "Iteration Number: 13600 | Cost: 0.9632 | Train Accuracy: 0.6406 | Validation Accuracy: 0.5082\n",
      "Iteration Number: 13700 | Cost: 0.9972 | Train Accuracy: 0.6250 | Validation Accuracy: 0.5008\n",
      "Iteration Number: 13800 | Cost: 1.0265 | Train Accuracy: 0.6172 | Validation Accuracy: 0.5072\n",
      "Iteration Number: 13900 | Cost: 1.0073 | Train Accuracy: 0.6797 | Validation Accuracy: 0.4986\n",
      "Iteration Number: 14000 | Cost: 0.9452 | Train Accuracy: 0.6719 | Validation Accuracy: 0.4926\n",
      "Iteration Number: 14100 | Cost: 0.9148 | Train Accuracy: 0.6484 | Validation Accuracy: 0.5012\n",
      "Iteration Number: 14200 | Cost: 0.8462 | Train Accuracy: 0.7422 | Validation Accuracy: 0.4986\n",
      "Iteration Number: 14300 | Cost: 1.0489 | Train Accuracy: 0.6250 | Validation Accuracy: 0.4952\n",
      "Iteration Number: 14400 | Cost: 0.9179 | Train Accuracy: 0.7109 | Validation Accuracy: 0.4982\n",
      "Iteration Number: 14500 | Cost: 1.0480 | Train Accuracy: 0.6328 | Validation Accuracy: 0.4802\n",
      "Iteration Number: 14600 | Cost: 1.0715 | Train Accuracy: 0.6484 | Validation Accuracy: 0.5024\n",
      "Iteration Number: 14700 | Cost: 0.9155 | Train Accuracy: 0.6641 | Validation Accuracy: 0.4986\n",
      "Iteration Number: 14800 | Cost: 1.0085 | Train Accuracy: 0.6250 | Validation Accuracy: 0.4982\n",
      "Iteration Number: 14900 | Cost: 1.0337 | Train Accuracy: 0.6094 | Validation Accuracy: 0.4928\n",
      "Iteration Number: 15000 | Cost: 0.9784 | Train Accuracy: 0.6641 | Validation Accuracy: 0.4944\n",
      "Iteration Number: 15100 | Cost: 1.0112 | Train Accuracy: 0.5859 | Validation Accuracy: 0.4658\n",
      "Iteration Number: 15200 | Cost: 1.1146 | Train Accuracy: 0.6328 | Validation Accuracy: 0.4856\n",
      "Iteration Number: 15300 | Cost: 0.9213 | Train Accuracy: 0.6953 | Validation Accuracy: 0.4918\n",
      "Iteration Number: 15400 | Cost: 1.1555 | Train Accuracy: 0.5781 | Validation Accuracy: 0.4938\n",
      "Iteration Number: 15500 | Cost: 1.1600 | Train Accuracy: 0.6172 | Validation Accuracy: 0.4938\n",
      "Iteration Number: 15600 | Cost: 0.9717 | Train Accuracy: 0.6797 | Validation Accuracy: 0.4944\n",
      "Iteration Number: 15700 | Cost: 1.2671 | Train Accuracy: 0.5781 | Validation Accuracy: 0.5008\n",
      "Iteration Number: 15800 | Cost: 1.2007 | Train Accuracy: 0.5625 | Validation Accuracy: 0.4588\n",
      "Iteration Number: 15900 | Cost: 1.1980 | Train Accuracy: 0.5234 | Validation Accuracy: 0.4804\n",
      "Iteration Number: 16000 | Cost: 0.9843 | Train Accuracy: 0.6641 | Validation Accuracy: 0.5004\n",
      "Iteration Number: 16100 | Cost: 0.9835 | Train Accuracy: 0.6562 | Validation Accuracy: 0.5046\n",
      "Iteration Number: 16200 | Cost: 0.8150 | Train Accuracy: 0.7656 | Validation Accuracy: 0.4920\n",
      "Iteration Number: 16300 | Cost: 0.8241 | Train Accuracy: 0.7188 | Validation Accuracy: 0.5044\n",
      "Iteration Number: 16400 | Cost: 0.9813 | Train Accuracy: 0.6328 | Validation Accuracy: 0.5012\n",
      "Iteration Number: 16500 | Cost: 1.0782 | Train Accuracy: 0.5703 | Validation Accuracy: 0.4902\n",
      "Iteration Number: 16600 | Cost: 1.1191 | Train Accuracy: 0.5703 | Validation Accuracy: 0.4876\n",
      "Iteration Number: 16700 | Cost: 0.8500 | Train Accuracy: 0.7344 | Validation Accuracy: 0.5072\n",
      "Iteration Number: 16800 | Cost: 0.7211 | Train Accuracy: 0.7031 | Validation Accuracy: 0.5018\n",
      "Iteration Number: 16900 | Cost: 0.8464 | Train Accuracy: 0.7344 | Validation Accuracy: 0.5016\n",
      "Iteration Number: 17000 | Cost: 0.8937 | Train Accuracy: 0.6797 | Validation Accuracy: 0.4916\n",
      "Iteration Number: 17100 | Cost: 0.9906 | Train Accuracy: 0.6641 | Validation Accuracy: 0.4938\n",
      "Iteration Number: 17200 | Cost: 0.9031 | Train Accuracy: 0.6484 | Validation Accuracy: 0.4952\n",
      "Iteration Number: 17300 | Cost: 0.6812 | Train Accuracy: 0.7578 | Validation Accuracy: 0.4946\n",
      "Iteration Number: 17400 | Cost: 0.7718 | Train Accuracy: 0.7188 | Validation Accuracy: 0.5038\n",
      "Iteration Number: 17500 | Cost: 0.9173 | Train Accuracy: 0.6953 | Validation Accuracy: 0.5058\n",
      "Iteration Number: 17600 | Cost: 0.7133 | Train Accuracy: 0.7734 | Validation Accuracy: 0.4928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Number: 17700 | Cost: 1.1588 | Train Accuracy: 0.5625 | Validation Accuracy: 0.4844\n",
      "Iteration Number: 17800 | Cost: 0.7329 | Train Accuracy: 0.7656 | Validation Accuracy: 0.5066\n",
      "Iteration Number: 17900 | Cost: 0.9517 | Train Accuracy: 0.6094 | Validation Accuracy: 0.5060\n",
      "Iteration Number: 18000 | Cost: 0.7904 | Train Accuracy: 0.7344 | Validation Accuracy: 0.5042\n",
      "Iteration Number: 18100 | Cost: 0.9333 | Train Accuracy: 0.5781 | Validation Accuracy: 0.4696\n",
      "Iteration Number: 18200 | Cost: 0.8145 | Train Accuracy: 0.6953 | Validation Accuracy: 0.5060\n",
      "Iteration Number: 18300 | Cost: 0.6872 | Train Accuracy: 0.7500 | Validation Accuracy: 0.5010\n",
      "Iteration Number: 18400 | Cost: 0.7881 | Train Accuracy: 0.7344 | Validation Accuracy: 0.5086\n",
      "Iteration Number: 18500 | Cost: 0.7920 | Train Accuracy: 0.6719 | Validation Accuracy: 0.5116\n",
      "Iteration Number: 18600 | Cost: 0.6874 | Train Accuracy: 0.7656 | Validation Accuracy: 0.4982\n",
      "Iteration Number: 18700 | Cost: 0.6918 | Train Accuracy: 0.7344 | Validation Accuracy: 0.4924\n",
      "Iteration Number: 18800 | Cost: 0.8280 | Train Accuracy: 0.7109 | Validation Accuracy: 0.4806\n",
      "Iteration Number: 18900 | Cost: 0.8409 | Train Accuracy: 0.7422 | Validation Accuracy: 0.4860\n",
      "Iteration Number: 19000 | Cost: 0.7322 | Train Accuracy: 0.7891 | Validation Accuracy: 0.5110\n",
      "Iteration Number: 19100 | Cost: 0.9534 | Train Accuracy: 0.7031 | Validation Accuracy: 0.5008\n",
      "Iteration Number: 19200 | Cost: 0.8003 | Train Accuracy: 0.7109 | Validation Accuracy: 0.4978\n",
      "Iteration Number: 19300 | Cost: 0.7985 | Train Accuracy: 0.7500 | Validation Accuracy: 0.4966\n",
      "Iteration Number: 19400 | Cost: 0.8606 | Train Accuracy: 0.7188 | Validation Accuracy: 0.4838\n",
      "Iteration Number: 19500 | Cost: 0.9092 | Train Accuracy: 0.6875 | Validation Accuracy: 0.4852\n",
      "Iteration Number: 19600 | Cost: 0.7951 | Train Accuracy: 0.7188 | Validation Accuracy: 0.5036\n",
      "Iteration Number: 19700 | Cost: 0.6698 | Train Accuracy: 0.7656 | Validation Accuracy: 0.5038\n",
      "Iteration Number: 19800 | Cost: 1.1914 | Train Accuracy: 0.5234 | Validation Accuracy: 0.4630\n",
      "Iteration Number: 19900 | Cost: 0.7918 | Train Accuracy: 0.7188 | Validation Accuracy: 0.4992\n"
     ]
    }
   ],
   "source": [
    "network.train(train_images, train_labels, alpha=0.01, iters=20000, batch_size=128, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBdgYy3b05Nh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5085\n"
     ]
    }
   ],
   "source": [
    "y_pred = network.predict(test_images)\n",
    "print(network.accuracy(y_pred,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zn4INedSpGrX"
   },
   "outputs": [],
   "source": [
    "np.save('ans1-uni.npy', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and check the results saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PankeHdJpPYd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 9 0 ... 5 5 7]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('ans1-uni.npy')\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "prashant.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
